---
title: "Casewise Likelihood and Scores"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Casewise Likelihood and Scores}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r dat}
library(semfindr)
dat <- pa_dat
```

```{r fit}
library(lavaan)
mod <-
"
m1 ~ iv1 + iv2
dv ~ m1
"
fit <- sem(mod, dat)
```

```{r fit_rerun}
if (file.exists("semfindr_fit_rerun.RDS")) {
    fit_rerun <- readRDS("semfindr_fit_rerun.RDS")
  } else {
    fit_rerun <- lavaan_rerun(fit)
    saveRDS(fit_rerun, "semfindr_fit_rerun.RDS")
  }
```

## Using Scores to Approximate Case Influence

`lavaan` provides the handy `lavScores()` functions to evaluate
$$s_i(\theta_m) = \frac{\partial \ell_i(\theta_m)}{\partial \theta_m}$$
for observation $i$, where $\ell_i(\theta)$ denotes the casewise loglikelihood function and $\theta_m$ is the $m$th model parameter.

For example, 

```{r lav-scores-head}
head(lavScores(fit)[ , 1, drop = FALSE])
```

indicates the derivative of the casewise loglikelihod with respect to the parameter `m1~iv1`. Because the sum of the scores is zero at the maximum likelihood estimate, $\hat \theta_m$, with the full sample (i.e., derivative of loglikelihood of the full data is 0), $- s_i(\theta_m)$ can be used as an estimate of the derivative of the loglikelihood at $\hat \theta_m$ **for the sample without observation $i$**. This information can be used to approximate the maximum likelihood estimate when case $i$ is dropped, denoted as $\hat \theta_{m(-i)}$

The second-order Taylor series expansion can be used to approximate the parameter vector estimate with an observation deleted, $\hat \theta_{(-i)}$, as in the iterative [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization). Specifically, 

$$\hat \theta_{(i)} \approx \hat \theta - \frac{N}{N - 1}V(\hat \theta) \nabla \ell(\hat \theta)$$
$$\hat \theta - \hat \theta_{(i)} = \frac{N}{N - 1}V(\hat \theta) \nabla \ell_i(\hat \theta),$$
where $\nabla \ell_i(\hat \theta)$ is the gradient vector of the casewise loglikelihood with respect to the parameters (i.e., score). The $N / (N - 1)$ term is used to adjust for the decrease in sample size (not 100% sure whether this is needed, but is trivial in large samples).

### Comparison

Here is a comparison between the approximation using scores from `lavaan::lavScore()` and `semfindr::est_change_raw()`

```{r compare-est-change}
# From semfindr
fit_est_change_raw <- est_change_raw(fit_rerun)
# From scores
fit_est_change_approx <- lavScores(fit) %*% vcov(fit) * 
  nobs(fit) / (nobs(fit) - 1)
# Plot the differences
library(ggplot2)
library(tidyr)
est_change_df <- 
  pivot_longer(as_tibble(fit_est_change_raw), cols = 1:5,
               names_to = "param", values_to = "est_change") |>
  cbind(est_change_approx = 
              pivot_longer(as_tibble(fit_est_change_approx),
                           cols = 1:5)$value)
ggplot(est_change_df, aes(x = est_change, y = est_change_approx)) +
  geom_abline(intercept = 0, slope = 1) +
  geom_point(size = 0.8, alpha = 0.5) +
  facet_wrap(~ param) +
  coord_fixed()
```

### Generalized Cook's distance (gCD)

We can use the approximate parameter change to approximate the gCD:

```{r approx-gcd}
# Hessian (inverse of covariance) with scale adjustment
information_fit <- lavInspect(fit, what = "information") * (nobs(fit) - 1)
# Short cut for computing quadratic form (https://stackoverflow.com/questions/27157127/efficient-way-of-calculating-quadratic-forms-avoid-for-loops)
gcd_approx <- rowSums(
  (fit_est_change_approx %*% information_fit) * fit_est_change_approx
)
# Compare to exact computation
fit_est_change <- est_change(fit_rerun)
# Plot
gcd_df <- data.frame(
  gcd_exact = fit_est_change[ , "gcd"],
  gcd_approx = gcd_approx
)
ggplot(gcd_df, aes(x = gcd_exact, y = gcd_approx)) +
  geom_abline(intercept = 0, slope = 1) +
  geom_point() +
  coord_fixed()
```

It seems that the approximation would underestimate the actual gCD, although the rank ordering is similar

```{r cor-gcd}
cor(gcd_df, method = "spearman")
```


## Approximate Change in Fit

The casewise loglikelihood in `lavaan` can approximate the change in loglikelihood when an observation is deleted:

```{r lli}
lli <- lavInspect(fit, what = "loglik.casewise")
head(lli)
```

Here, $\ell(\hat \theta)$ will drop 2.78 when observation 1 is deleted. This should approximate $\ell(\hat \theta_{(-i)})$ as long as $\hat \theta_{(-i)}$ is not too different from $\hat \theta$. Here's a comparison:

```{r compare-lli}
# Predicted ll without observation 1
fit@loglik$loglik - lli[1]
# Actual ll without observation 1
fit_no1 <- sem(mod, dat[-1, ])
fit_no1@loglik$loglik
```

They are pretty close. To compute the change in chi-squared, however, the predicted loglikelihood of the saturated (h1) model is also needed. I am not sure whether `lavaan` can compute casewise likelihood for the saturated model, but this can be done manually:

```{r fit_h1}
mod_h1 <-
"
m1 ~ iv1 + iv2
dv ~ m1 + iv1 + iv2
"
fit_h1 <- sem(mod_h1, dat)
lli_h1 <- lavInspect(fit_h1, what = "loglik.casewise")
```

As the model chi-squared is the difference between -2 x h1 loglikelihood and -2 x loglikelihood of specified model, we can obtain the predicted chi-squared with an observation dropped as

```{r chisq_i_approx}
ll_approx <- fit@loglik$loglik - lli  # predicted model ll
ll_h1_approx <- fit_h1@loglik$loglik - lli_h1  # predicted h1 ll
chisq_i_approx <- 2 * (ll_h1_approx - ll_approx)
# Compare to the actual chisq when dropping observation 1
c(predict = chisq_i_approx[1],
  actual = fitmeasures(fit_no1, "chisq"))
```

### Comparison

```{r plot-change-chisq}
# Below is a shortcut as
# chisq == -2 * (fit_h1@loglik$loglik - fit@loglik$loglik)
chisq_change_i_approx <- 2 * (lli_h1 - lli)
# From semfindr
out <- fit_measures_change(fit_rerun)
# Plot
chisq_change_df <- data.frame(
  chisq_change = out[ , "chisq"],
  chisq_change_approx = chisq_change_i_approx
)
ggplot(chisq_change_df, aes(x = chisq_change, y = chisq_change_approx)) +
  geom_abline(intercept = 0, slope = 1) +
  geom_point() +
  coord_fixed()
```


